{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs required\n",
    "\n",
    "* customers_FR.csv - Customers data after Feature Reduction\n",
    "* unknown_values.csv - Levels that should be coded as NULL, from Data Dictionary\n",
    "* mailout_train_out.csv - A subset of the train data with only the columns that have been found after FR on Customers data. ~60 features.\n",
    "* mailout_test.csv - A subset of the test data with only the columns that have been found after FR on Customers data. ~60 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. I looked through the Data Dictionary to decide whether it makes sense to treat the features as Oridinal or Nominal. Oridinal features don't need to be encoded in this case but need to be careful with choice of alogrithm. I'm choosing tree based algorithms because they will choose a cut-off point and won't use the absolute value. i.e. don't want it to treat 2 as double of 1. But need to be careful as high cardinality leads to poor performance of tree-based models - assuming we will be ok with 2-4 features being encoded with dummies.\n",
    "2. The columns prefixed KBA are ~20% missing so I have segmented the between Customers with this data and Customers without.\n",
    "3. Using GridsearchCV on RandomForestClassifier I have fitted 2 models and printed the AUC of both.\n",
    "\n",
    "#### Reference\n",
    "Follow the steps here includes one-hot-encoding: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: plotnine in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (1.4.2)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (1.22.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (0.5.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (1.5.3)\n",
      "Requirement already satisfied: statsmodels>=0.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (0.13.1)\n",
      "Requirement already satisfied: mizani>=0.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (0.7.4)\n",
      "Requirement already satisfied: descartes>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from plotnine) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (4.28.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from matplotlib>=3.1.1->plotnine) (21.3)\n",
      "Requirement already satisfied: palettable in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from mizani>=0.7.3->plotnine) (3.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas>=1.1.0->plotnine) (2022.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from patsy>=0.5.1->plotnine) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from plotnine import *\n",
    "import plotnine\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Import module for k-modes cluster\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline\n",
    "# Use the theme of ggplot\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "pd.set_option('display.max_rows', 999)\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify columns\n",
    "\n",
    "# Numeric\n",
    "list_numeric  = [\n",
    "    'ANZ_HH_TITEL'\n",
    "    ,'ANZ_PERSONEN'\n",
    "#     ,'ANZ_TITEL'\n",
    "    , 'MIN_GEBAEUDEJAHR' # This is year so needs to be recoded to Time since\n",
    "    ]\n",
    "\n",
    "# greater than 100 levels - likely numeric\n",
    "list_large_lvls  = [\n",
    "    'LNR'                            \n",
    "    ,'EINGEFUEGT_AM'                    \n",
    "    ,'KBA13_ANZAHL_PKW'                 \n",
    "    ,'ANZ_HAUSHALTE_AKTIV'               \n",
    "    ,'ANZ_STATISTISCHE_HAUSHALTE'        \n",
    "    ,'GEBURTSJAHR'     \n",
    "    ]\n",
    "\n",
    "# fields with large number of missings\n",
    "list_missing  = [\n",
    "'ALTER_KIND4'                    \n",
    ",'ALTER_KIND3'                   \n",
    ",'TITEL_KZ'                       \n",
    ",'ALTER_KIND2'                  \n",
    ",'ALTER_KIND1'                  \n",
    ",'KK_KUNDENTYP'                \n",
    ",'KBA05_BAUMAX'                  \n",
    ",'AGER_TYP'                      \n",
    ",'EXTSEL992'                     \n",
    ",'ALTER_HH'                      \n",
    ",'W_KEIT_KIND_HH' \n",
    "  ]\n",
    "\n",
    "# Columns with single level with my that 98% of customers in 1 of the levels. Based on clean data after removing 05/13, Na rows\n",
    "list_high_sing_lvl = [\n",
    "'ANZ_TITEL'\n",
    ",'D19_TELKO_ONLINE_DATUM'\n",
    ",'D19_TELKO_ONLINE_QUOTE_12'\n",
    ",'D19_VERSI_ONLINE_DATUM'\n",
    ",'D19_VERSI_ONLINE_QUOTE_12'\n",
    ",'DSL_FLAG'\n",
    ",'SOHO_KZ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_types(customers_csv, list_large_lvls, list_numeric):\n",
    "    \"\"\"\n",
    "    Load in customers data and creates a dictionary with the columns that should be coded as object. \n",
    "    \"\"\"\n",
    "    \n",
    "    all_columns = pd.read_csv(customers_csv, nrows=0 , sep=';').columns.tolist()\n",
    "\n",
    "    object_vars = list(set(all_columns) - set(list_large_lvls) - set(list_numeric))\n",
    "\n",
    "    dic_dtype = {}\n",
    "    for var in object_vars:\n",
    "        dic_dtype.update({var: 'object'})\n",
    "        \n",
    "    return dic_dtype\n",
    "\n",
    "dic_dtype = data_types('customers_FR.csv', list_large_lvls\n",
    "                      , list_numeric )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_miss_data(missing_csv):\n",
    "    \"\"\"\n",
    "    Load in missing data info and outputs 2 dictionaries of the level that should be NULL.\n",
    "    \"\"\"\n",
    "    missing_values = pd.read_csv(missing_csv)\n",
    "    \n",
    "    dic_miss = dict(missing_values[['Attribute','Value']].values)\n",
    "    dic_miss2 = dict(missing_values[['Attribute','second']].dropna().values)\n",
    "    \n",
    "    return dic_miss, dic_miss2\n",
    "\n",
    "dic_miss, dic_miss2 = load_miss_data('unknown_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42962, 61)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data and drop columns which are no good\n",
    "mailout_train = pd.read_csv(\"mailout_train_out.csv\" ,\n",
    "                       dtype=dic_dtype\n",
    "                      )\n",
    "mailout_train.drop(columns=['RESPONSE.1', 'D19_VERSI_ANZ_12', 'D19_BANKEN_ONLINE_DATUM', 'D19_TELKO_DATUM' ], inplace=True)\n",
    "mailout_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Missing categories as Nulls\n",
    "for key, value in dic_miss.items():\n",
    "    try:\n",
    "        mailout_train[key]=mailout_train[key].apply(lambda x: np.nan if x==value else x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for key, value in dic_miss2.items():\n",
    "    try:\n",
    "        mailout_train[key]=mailout_train[key].apply(lambda x: np.nan if x==value else x)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by Missing KBA13 and KBA05\n",
    "\n",
    "mailout_train['KBA05_Missing'] = mailout_train['KBA13_KW_110'].isnull()\n",
    "mailout_train['KBA13_Missing'] = mailout_train['KBA05_HERST5'].isnull()\n",
    "\n",
    "train_notmiss    =  mailout_train[(mailout_train['KBA05_Missing'] == False) & (mailout_train['KBA13_Missing'] == False)].drop(['KBA05_Missing', 'KBA13_Missing'], axis = 1)\n",
    "train_miss =  mailout_train[(mailout_train['KBA05_Missing'] == True)  | (mailout_train['KBA13_Missing'] == True)].drop(['KBA05_Missing', 'KBA13_Missing'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.987706\n",
       "1    0.012294\n",
       "Name: RESPONSE, dtype: float64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_notmiss.RESPONSE.value_counts()/train_notmiss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.987288\n",
       "1    0.012712\n",
       "Name: RESPONSE, dtype: float64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_miss.RESPONSE.value_counts()/train_miss.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all Customers with Missing values\n",
    "train_notmiss.dropna(inplace = True)\n",
    "\n",
    "# Features in the KBA Miss Segment\n",
    "train_KBAMiss_feat = [\n",
    "'RESPONSE'                         , \n",
    "'D19_VERSI_OFFLINE_DATUM'          , \n",
    "'FINANZ_VORSORGER'                 , \n",
    "'SEMIO_VERT'                      ,  \n",
    "'CJT_GESAMTTYP'                   , \n",
    "'RETOURTYP_BK_S'                  , \n",
    "'LP_FAMILIE_GROB'                 , \n",
    "'HH_EINKOMMEN_SCORE'              \n",
    "]\n",
    "\n",
    "# Keep columns that are well populated\n",
    "train_miss_cols = train_miss[train_KBAMiss_feat].copy()\n",
    "# Impute since low volumes\n",
    "train_miss_cols[train_KBAMiss_feat] = train_miss_cols[train_KBAMiss_feat].fillna(train_miss_cols[train_KBAMiss_feat].mode().iloc[0])\n",
    "# Or drop\n",
    "# train_miss_cols.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up lists of features\n",
    "nominal_miss = ['LP_FAMILIE_GROB', 'CJT_GESAMTTYP' ]\n",
    "nominal_notmiss = ['LP_FAMILIE_GROB', 'CJT_GESAMTTYP', 'GEBAEUDETYP', 'NATIONALITAET_KZ' ]\n",
    "\n",
    "ordinal_miss = list(set(train_KBAMiss_col.columns) - set(nominal_miss) - set(['RESPONSE']))\n",
    "ordinal_notmiss = list(set(train_KBAnotMiss.columns) - set(nominal_notmiss) - set(['RESPONSE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_y(df, ordin, nom, create_y):\n",
    "\n",
    "    OR = df[ordin]\n",
    "    NOM = df[nom]\n",
    "    if create_y == 1:\n",
    "        y = df['RESPONSE']\n",
    "    else:\n",
    "        y= 0\n",
    "\n",
    "    # One-hot encode the data using pandas get_dummies\n",
    "    features = pd.get_dummies(NOM, drop_first = True)\n",
    "    # Join everything back again\n",
    "    X_df = pd.concat([features, OR], axis = 1)\n",
    "\n",
    "    # Convert to numpy array\n",
    "    X = np.array(X_df)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "Xnotmiss, ynotmiss = create_X_y(train_notmiss, ordinal_notmiss, nominal_notmiss,1)\n",
    "Xmiss, ymiss = create_X_y(train_miss_cols, ordinal_miss, nominal_miss,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [5, 6, 7, 8, 9, 10],\n",
       "                         'min_samples_split': [2, 4, 10],\n",
       "                         'n_estimators': [100, 150, 200], 'random_state': [0]})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit models on each segment\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators'      : [100,150,200],\n",
    "    'max_depth'         : [5, 6, 7, 8, 9, 10],\n",
    "    'random_state'      : [0],\n",
    "    'min_samples_split': [2,4,10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "clfmiss = GridSearchCV(RandomForestClassifier(), parameters, cv=5)\n",
    "clfnotmiss = GridSearchCV(RandomForestClassifier(), parameters, cv=5)\n",
    "\n",
    "clfmiss.fit(Xmiss, ymiss)\n",
    "clfnotmiss.fit(Xnotmiss, ynotmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6560752465924881\n",
      "0.9031300529937327\n"
     ]
    }
   ],
   "source": [
    "# print AUC on Training\n",
    "print(roc_auc_score(ymiss, clfmiss.predict_proba(Xmiss)[:, 1]) )\n",
    "print(roc_auc_score(ynotmiss, clfnotmiss.predict_proba(Xnotmiss)[:, 1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot feature importances ###\n",
    "\n",
    "# importances = clfnotmiss.feature_importances_\n",
    "# std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "# forest_importances.sort_values(ascending=False)\n",
    "\n",
    "# forest_importances = pd.Series(importances, index=feature_names)\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "# ax.set_title(\"Feature importances using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter.\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer – this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Load in the test data and recode missing values as Nulls.\n",
    "2. Segment the data between missing KBA columns and those which have these columns populated.\n",
    "3. Impute missing with the mode so everyone gets a score.\n",
    "4. Score and then join back together. \n",
    "5. Output to csv and then upload to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42833, 61)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in and prep data\n",
    "\n",
    "mailout_test = pd.read_csv(\"mailout_test.csv\" ,\n",
    "                       dtype=dic_dtype\n",
    "                      )\n",
    "mailout_test.drop(columns=[ 'D19_VERSI_ANZ_12', 'D19_BANKEN_ONLINE_DATUM', 'D19_TELKO_DATUM' ], inplace=True)\n",
    "mailout_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode Missing categories as Nulls\n",
    "for key, value in dic_miss.items():\n",
    "    try:\n",
    "        mailout_test[key]=mailout_test[key].apply(lambda x: np.nan if x==value else x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for key, value in dic_miss2.items():\n",
    "    try:\n",
    "        mailout_test[key]=mailout_test[key].apply(lambda x: np.nan if x==value else x)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment by Missing KBA13 and KBA05\n",
    "# Drop customers with missing KBA05 and KBA13 data\n",
    "mailout_test['KBA05_Missing'] = mailout_test['KBA13_KW_110'].isnull()\n",
    "mailout_test['KBA13_Missing'] = mailout_test['KBA05_HERST5'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment data\n",
    "test_notmiss  =  mailout_test[(mailout_test['KBA05_Missing'] == False)  & (mailout_test['KBA13_Missing'] == False)].drop(['KBA05_Missing', 'KBA13_Missing'], axis = 1)\n",
    "test_miss     =  mailout_test[(mailout_test['KBA05_Missing'] == True)  | (mailout_test['KBA13_Missing'] == True)].drop(['KBA05_Missing', 'KBA13_Missing'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# immute missing values \n",
    "def imp_modes(df, list_vars):\n",
    "    df[list_vars] = df[list_vars].fillna(df[list_vars].mode().iloc[0])\n",
    "    \n",
    "imp_modes(test_notmiss, ordinal_notmiss)\n",
    "imp_modes(test_notmiss, nominal_notmiss)\n",
    "imp_modes(test_miss, ordinal_miss)\n",
    "imp_modes(test_miss, nominal_miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testnotmiss, y_testnotmiss = create_X_y(test_notmiss, ordinal_notmiss, nominal_notmiss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testmiss, y_testmiss = create_X_y(test_miss, ordinal_miss, nominal_miss, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pred(X, clf, df):\n",
    "    pred = clf.predict_proba(X)\n",
    "    kaggle= df[['LNR']].copy()\n",
    "    kaggle['RESPONSE'] = pred[:,1]\n",
    "    \n",
    "    return kaggle\n",
    "\n",
    "kaggle_miss = add_pred(X_testmiss, clfmiss, test_miss)\n",
    "kaggle_notmiss = add_pred(X_testnotmiss, clfnotmiss, test_notmiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.concat([kaggle_miss, kaggle_notmiss], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42833, 2)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "kaggle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012601929397830343"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check\n",
    "kaggle.RESPONSE.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pandas==1.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas==1.3.0) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas==1.3.0) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from pandas==1.3.0) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Collecting numpy>=1.18.5\n",
      "  Downloading numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, six, numpy, python-dateutil, pandas\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2021.3\n",
      "    Uninstalling pytz-2021.3:\n",
      "      Successfully uninstalled pytz-2021.3\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.3\n",
      "    Uninstalling numpy-1.20.3:\n",
      "      Successfully uninstalled numpy-1.20.3\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.8.2\n",
      "    Uninstalling python-dateutil-2.8.2:\n",
      "      Successfully uninstalled python-dateutil-2.8.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.0\n",
      "    Uninstalling pandas-1.3.0:\n",
      "      Successfully uninstalled pandas-1.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "sagemaker-pyspark 1.4.2 requires pyspark==2.4.0, but you have pyspark 3.0.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\n",
      "boto3 1.21.42 requires botocore<1.25.0,>=1.24.42, but you have botocore 1.24.19 which is incompatible.\n",
      "awscli 1.22.97 requires botocore==1.24.42, but you have botocore 1.24.19 which is incompatible.\n",
      "aiobotocore 2.0.1 requires botocore<1.22.9,>=1.22.8, but you have botocore 1.24.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.4 pandas-1.4.2 python-dateutil-2.8.2 pytz-2022.1 six-1.16.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.3.0\n",
    "!pip install --force-reinstall pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle.to_csv(\"kaggle_segment.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest Kaggle Score: 0.54526"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
